{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARXIV CONTENT RETRIEVER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Build an Arxiv paper content retriever](https://nayakpplaban.medium.com/build-an-arxiv-paper-content-retriever-and-summarizer-agent-using-completely-local-openai-swarm-cc03afeecef1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qwen2.5:3b'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "model = os.getenv(\"LLM_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from swarm import Swarm, Agent\n",
    "\n",
    "ollama_client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\", api_key=\"ollama\"  # required but unused\n",
    ")\n",
    "#\n",
    "client = Swarm(client=ollama_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper function to return axriv paper details based on the topic provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_url_topic(topic):\n",
    "    # Prompt user for the topic to search\n",
    "    print(topic)\n",
    "    # topic = \"ChunkRag\"\n",
    "    # Set up the search parameters\n",
    "    search = arxiv.Search(\n",
    "        query=topic,\n",
    "        max_results=1,  # You can adjust this number as needed\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "        sort_order=arxiv.SortOrder.Descending,\n",
    "    )\n",
    "\n",
    "    # Prepare a list to store results\n",
    "    all_data = []\n",
    "\n",
    "    # Execute the search and collect results\n",
    "    for result in search.results():\n",
    "        # print(result)\n",
    "        paper_info = {\n",
    "            \"Title\": result.title,\n",
    "            \"Date\": result.published.date(),\n",
    "            \"Id\": result.entry_id,\n",
    "            \"Summary\": result.summary,\n",
    "            \"URL\": result.pdf_url,\n",
    "        }\n",
    "        all_data.append(paper_info)\n",
    "\n",
    "    if all_data:\n",
    "        results = \"\\n\\n\".join(\n",
    "            [\n",
    "                f\"Title:{d['Title']}\\nDate:{d['Date']}\\nURL:{d['URL']}\\nSummary:{d['Summary']}\"\n",
    "                for d in all_data\n",
    "            ]\n",
    "        )\n",
    "        # print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkRag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/43/572h075x24q9rq1slmdfw9yw0000gn/T/ipykernel_99716/1531379633.py:21: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems\n",
      "Date:2024-10-25\n",
      "URL:http://arxiv.org/pdf/2410.19572v4\n",
      "Summary:Retrieval-Augmented Generation (RAG) systems using large language models\n",
      "(LLMs) often generate inaccurate responses due to the retrieval of irrelevant\n",
      "or loosely related information. Existing methods, which operate at the document\n",
      "level, fail to effectively filter out such content. We propose LLM-driven chunk\n",
      "filtering, ChunkRAG, a framework that enhances RAG systems by evaluating and\n",
      "filtering retrieved information at the chunk level. Our approach employs\n",
      "semantic chunking to divide documents into coherent sections and utilizes\n",
      "LLM-based relevance scoring to assess each chunk's alignment with the user's\n",
      "query. By filtering out less pertinent chunks before the generation phase, we\n",
      "significantly reduce hallucinations and improve factual accuracy. Experiments\n",
      "show that our method outperforms existing RAG models, achieving higher accuracy\n",
      "on tasks requiring precise information retrieval. This advancement enhances the\n",
      "reliability of RAG systems, making them particularly beneficial for\n",
      "applications like fact-checking and multi-hop reasoning.\n"
     ]
    }
   ],
   "source": [
    "print(get_url_topic(\"ChunkRag\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper function to extract content of arxiv paper based on the url provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from langchain_ollama import ChatOllama\n",
    "from arxiv2text import arxiv_to_text\n",
    "from openai import OpenAI\n",
    "\n",
    "ollama_client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\", api_key=\"ollama\"  # required but unused\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:1b\", temperature=0, num_predict=1000)\n",
    "\n",
    "\n",
    "class SummarizerAgent:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOllama(model=\"llama3.2:1b\", temperature=0.0, num_predict=1000)\n",
    "\n",
    "    def extract_content(self, url: str) -> str:\n",
    "        # Replace with your specific arXiv PDF URL\n",
    "        pdf_url = url\n",
    "        extracted_text = arxiv_to_text(pdf_url)\n",
    "        return extracted_text\n",
    "\n",
    "    def summarize_paper(self, paper: Dict, content: str) -> str:\n",
    "        \"\"\"\n",
    "        Summarize a single paper using Llama2\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Please provide a concise summary of the following research paper:\n",
    "        Title: {paper['title']}\n",
    "        Authors: {', '.join(paper['authors'])}\n",
    "        Abstract: {paper['summary']}\n",
    "        Content : {content}\n",
    "\n",
    "        Generate a clear ,concise and informative summary in no more than 6-8 sentences.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.llm.predict(prompt)\n",
    "\n",
    "    def summarize_papers(self, papers: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Summarize multiple papers\n",
    "        \"\"\"\n",
    "        summarized_papers = []\n",
    "        for paper in papers:\n",
    "            summary = self.summarize_paper(paper)\n",
    "            summarized_papers.append(\n",
    "                {\"title\": paper[\"title\"], \"summary\": summary, \"original_paper\": paper}\n",
    "            )\n",
    "\n",
    "        return summarized_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test the helper function\n",
    "\n",
    "\n",
    "# def extract_content(url):\n",
    "#     summ = SummarizerAgent()\n",
    "#     content = summ.extract_content(url)\n",
    "#     return content\n",
    "\n",
    "\n",
    "# content = extract_content(\"https://arxiv.org/pdf/2502.13966v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the helper function\n",
    "\n",
    "\n",
    "def summarize_extracted_content(url):\n",
    "    summ = SummarizerAgent()\n",
    "    content = summ.extract_content(url)\n",
    "    return content\n",
    "\n",
    "\n",
    "content = extract_content(\"https://arxiv.org/pdf/2502.13966v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='2 + 2 = 4', additional_kwargs={}, response_metadata={'model': 'llama3.2:latest', 'created_at': '2025-02-21T01:57:56.685224Z', 'done': True, 'done_reason': 'stop', 'total_duration': 958943166, 'load_duration': 34555791, 'prompt_eval_count': 31, 'prompt_eval_duration': 845000000, 'eval_count': 8, 'eval_duration': 78000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-57d47a04-d0d2-47d7-94ad-075d4c8b6bb7-0', usage_metadata={'input_tokens': 31, 'output_tokens': 8, 'total_tokens': 39})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOllama(model=\"llama3.2:latest\", temperature=0, num_predict=1000)\n",
    "llm.invoke(\"what is 2+2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create URL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_agent = Agent(\n",
    "    name=\"Extract URL Assistant\",\n",
    "    instruction=\"Get the arxiv search results for the given topic.\",\n",
    "    functions=[get_url_topic],\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create extract url agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_url = Agent(\n",
    "    name=\"URL Assistant\", instruction=\"Get the URL from the given content.\", model=model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Summary Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_agent = Agent(\n",
    "    name=\"Extract Summary Assistant\",\n",
    "    instruction=\"\"\"Generate a clear ,concise and informative summary of the arxiv paper.The Summary should include the authors of the paper , the date it was published and\n",
    "                  the concept behind the topic explained i the paper.\"\"\",\n",
    "    functions=[extract_content],\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Summary Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Where’s the Bug? Attention Probing for Scalable Fa'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I understand that you're providing additional information and results related to your study on feedback loop (FL) error analysis using prompting methods for natural language processing. Here's a refined version with some formatting adjustments and clarity improvements:\n",
      "\n",
      "---\n",
      "\n",
      "### Supplementary Information\n",
      "\n",
      "#### B. Additional Results\n",
      "\n",
      "##### B.1 Error Bars for FL Results\n",
      "\n",
      "Error bars are typically used to visualize the variability or uncertainty around the mean of results. For the FL error analysis presented in Table 2, the following table provides additional information on error margins:\n",
      "\n",
      "| Metric | Prompting Method (Accuracy) |\n",
      "|--------|--------------------------------|\n",
      "| D4J    | 0.144 ± X                     |\n",
      "| GH-Py  |                                  | \n",
      "| N/A    |                                  |  \n",
      "| GH-J   | 0.375 (N/A, No result available)|  \n",
      "| DeepFix|R/No data                      |\n",
      "| TSSB   | R/R                            |\n",
      "| MS4J   | 0.169                          |  \n",
      "| JulietJ|0.185 ± X                     |\n",
      "| WLLC-CodeBERT| /No data                  |\n",
      "\n",
      "Where **R** indicates no result and the error bar format (±X, e.g., ±0.2) can be added for more detailed analysis if available.\n",
      "\n",
      "##### B.2 Precision at k\n",
      "\n",
      "A common metric in FL error analysis is the \"Precision at k\" or P@k. It measures the fraction of buggy lines correctly identified out of top k candidate explanations.\n",
      "\n",
      "The precision at k (P@k) is defined as:\n",
      "\n",
      "\\[ \\text{Correct in top k} / \\min(k, \\text{Max possible correct})\\]\n",
      "\n",
      "This metric is useful when there are fewer buggy lines than expected. For example, if k=5 and the maximal number of bugs could be identified but there were only 2-3 actual buggy lines, the denominator effectively accounts for this scenario.\n",
      "\n",
      "#### C. Detailed Analysis\n",
      "\n",
      "As part of our study on feedback loop error analysis using prompting methods for natural language processing (NLP), we have conducted a thorough evaluation to understand how various methods perform. Below are additional details and results that provide deeper insights into our findings:\n",
      "\n",
      "---\n",
      "\n",
      "This format improves readability and makes the supplementary information clearer, providing additional context for your paper's readers.\n",
      "\n",
      "---\n",
      "\n",
      "Feel free to let me know if you need any further adjustments or adding more content! This framework helps in presenting detailed supplementary information concisely.\n"
     ]
    }
   ],
   "source": [
    "summary_response = client.run(\n",
    "    agent=content_agent, messages=[{\"role\": \"user\", \"content\": content}]\n",
    ")\n",
    "\n",
    "print(summary_response.messages[-1][\"content\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
