{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Agentic System to enhance RAG with Self-Grading and Web Search Capabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [medium](https://medium.com/the-ai-forum/building-an-agentic-system-to-enhance-rag-with-self-grading-and-web-search-capabilities-using-3f9a1d885730)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set up the API keys in google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")\n",
    "os.environ[\"LOGFIRE_IGNORE_NO_CONFIG\"] = os.getenv(\"LOGFIRE_IGNORE_NO_CONFIG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Instantiate LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "from pydantic_ai.models.groq import GroqModel\n",
    "\n",
    "openai_model = OpenAIModel(\"gpt-4o-mini\")\n",
    "# groq_model = GroqModel(\"mixtral-8x7b-32768\")  # or another valid Groq model\n",
    "groq_model = GroqModel(\"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load required documents and build index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/43/572h075x24q9rq1slmdfw9yw0000gn/T/ipykernel_89550/890859462.py:14: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceBgeEmbeddings(\n",
      "/Users/vamsi_mbmax/Library/CloudStorage/OneDrive-Personal/01_vam_PROJECTS/LEARNING/proj_AI/dev_proj_AI/practise_ai_misc_projects/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from langchain_community.vectorstores import chroma\n",
    "from langchain_chroma.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(\"../data/RAG.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "split_docs = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=50\n",
    ").split_documents(documents)\n",
    "\n",
    "embedding = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "persist_directory = \"../data/chroma_langchain_db\"\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory,\n",
    "    collection_name=\"RAG_vectorstore\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataclasses import dataclass\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "# class Deps:\n",
    "#     question: str | None\n",
    "#     context: str | None\n",
    "#     query: str | None  # Added as it's used in system prompt\n",
    "#     response: str | None  # Added as it's used in system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "# 1. Define our Deps class correctly\n",
    "class Deps(BaseModel):\n",
    "    question: str = Field(default=\"\")\n",
    "    query: str = Field(default=\"\")\n",
    "    context: Optional[str] = Field(default=\"\")\n",
    "    response: Optional[str] = Field(default=\"\")\n",
    "    content: str = Field(default=\"\")\n",
    "    message: str = Field(default=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Instantiate pydantic.ai Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a Helpful Assistant proficient in providing concise, factual, and to-the-point answers for questions about RAG based on provided context.\n",
    "\n",
    "When answering questions:\n",
    "1. First use the `retriever_tool` to get relevant context from the knowledge base\n",
    "2. If the retrieved context is insufficient, use the `websearch_tool` to get additional information\n",
    "3. Generate a comprehensive response based on all available context\n",
    "\n",
    "Your response should be in JSON format with the following structure:\n",
    "{\n",
    "    \"Relevancy\": <score 0-1>,\n",
    "    \"Faithfulness\": <score 0-1>,\n",
    "    \"Context Quality\": <score 0-1>,\n",
    "    \"Needs Web Search\": True,  # Set to True when websearch_tool was used, False if not\n",
    "    \"Explanation\": \"Explanation of the grading and search decision\",\n",
    "    \"Answer\": \"Your detailed answer based on the available context\"\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "groq_agent = Agent(\n",
    "    groq_model,\n",
    "    deps_type=Deps,\n",
    "    retries=2,\n",
    "    result_type=str,\n",
    "    system_prompt=system_prompt,\n",
    "    end_strategy=\"early\",  # This is the default but being explicit\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7,8.Function Tools - Web search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "\n",
    "\n",
    "@groq_agent.tool_plain\n",
    "async def websearch_tool(question: str) -> str:\n",
    "    \"\"\"Search the web for information\"\"\"\n",
    "    try:\n",
    "        tavily_client = TavilyClient()\n",
    "        answer = tavily_client.qna_search(query=question)\n",
    "        print(\n",
    "            f\"Web search result: {str(answer)[:200]}...\"\n",
    "        )  # Print first 200 chars for debugging\n",
    "        return str(answer)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in websearch_tool: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"what time is the match between intermiami, new york. is messi playing\"\n",
    "\n",
    "# tavily_client = TavilyClient()\n",
    "\n",
    "# # Step 2. Executing a Q&A search query\n",
    "# answer = tavily_client.qna_search(query=question)\n",
    "\n",
    "# # Step 3. That's it! Your question has been answered!\n",
    "# print(f\"WEB SEARCH:{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create Retriever Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/chroma_langchain_db'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persist_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import RunContext\n",
    "from typing import List\n",
    "\n",
    "\n",
    "@groq_agent.tool\n",
    "async def retriever_tool(ctx: RunContext[Deps], question: str) -> str:\n",
    "    \"\"\"Retrieve relevant documents from the vector store\"\"\"\n",
    "    try:\n",
    "        load_vectorstore = Chroma(\n",
    "            persist_directory=persist_directory,\n",
    "            embedding_function=embedding,\n",
    "            collection_name=\"RAG_vectorstore\",\n",
    "        )\n",
    "        retrieved_docs = load_vectorstore.similarity_search(query=question, k=3)\n",
    "        context = \"\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "        print(\n",
    "            f\"Retrieved context: {context[:200]}...\"\n",
    "        )  # Print first 200 chars for debugging\n",
    "        return context\n",
    "    except Exception as e:\n",
    "        print(f\"Error in retriever_tool: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Results â€” Invoke the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question: what is a RAG\n",
    "# query = \"What is RAG?\"\n",
    "# response = groq_agent.run_sync(\n",
    "#     # Deps(question=query, query=query)\n",
    "#     # Deps(question=query, query=query, context=None, response=None)\n",
    "# )\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved context: The design also incorporates the feedback from a diverse group of partici-\n",
      "pants during a workshop session, which focused on the practical aspects of imple-\n",
      "menting RAG systems. Their input highlighte...\n",
      "Web search result: Retrieval Augmented Generation (RAG) encompasses various techniques that enhance language models by integrating external knowledge sources. Traditional RAG models retrieve relevant information from a ...\n",
      "Response: RunResult(_all_messages=[ModelRequest(parts=[SystemPromptPart(content='You are a Helpful Assistant proficient in providing concise, factual, and to-the-point answers for questions about RAG based on provided context.\\n\\nWhen answering questions:\\n1. First use the `retriever_tool` to get relevant context from the knowledge base\\n2. If the retrieved context is insufficient, use the `websearch_tool` to get additional information\\n3. Generate a comprehensive response based on all available context\\n\\nYour response should be in JSON format with the following structure:\\n{\\n    \"Relevancy\": <score 0-1>,\\n    \"Faithfulness\": <score 0-1>,\\n    \"Context Quality\": <score 0-1>,\\n    \"Needs Web Search\": True,  # Set to True when websearch_tool was used, False if not\\n    \"Explanation\": \"Explanation of the grading and search decision\",\\n    \"Answer\": \"Your detailed answer based on the available context\"\\n}\\n', dynamic_ref=None, part_kind='system-prompt'), UserPromptPart(content='how many types of RAG are available in retrieval augmented generation? how can we use Agentic RAG in production?', timestamp=datetime.datetime(2025, 2, 22, 22, 27, 15, 970896, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'), ModelResponse(parts=[ToolCallPart(tool_name='retriever_tool', args='{\"question\": \"types of RAG in retrieval augmented generation and Agentic RAG in production\"}', tool_call_id='call_vccx', part_kind='tool-call')], model_name='llama-3.3-70b-versatile', timestamp=datetime.datetime(2025, 2, 22, 22, 27, 16, tzinfo=datetime.timezone.utc), kind='response'), ModelRequest(parts=[ToolReturnPart(tool_name='retriever_tool', content='The design also incorporates the feedback from a diverse group of partici-\\npants during a workshop session, which focused on the practical aspects of imple-\\nmenting RAG systems. Their input highlighted the effectiveness of the systemâ€™s\\nreal-time retrieval capabilities, particularly in knowledge-intensive domains, and\\nunderscored the importance of refining the integration between retrieval and\\ngeneration to enhance the transparency and reliability of the systemâ€™s outputs.\\nThe design also incorporates the feedback from a diverse group of partici-\\npants during a workshop session, which focused on the practical aspects of imple-\\nmenting RAG systems. Their input highlighted the effectiveness of the systemâ€™s\\nreal-time retrieval capabilities, particularly in knowledge-intensive domains, and\\nunderscored the importance of refining the integration between retrieval and\\ngeneration to enhance the transparency and reliability of the systemâ€™s outputs.\\nThe design also incorporates the feedback from a diverse group of partici-\\npants during a workshop session, which focused on the practical aspects of imple-\\nmenting RAG systems. Their input highlighted the effectiveness of the systemâ€™s\\nreal-time retrieval capabilities, particularly in knowledge-intensive domains, and\\nunderscored the importance of refining the integration between retrieval and\\ngeneration to enhance the transparency and reliability of the systemâ€™s outputs.', tool_call_id='call_vccx', timestamp=datetime.datetime(2025, 2, 22, 22, 27, 16, 614098, tzinfo=datetime.timezone.utc), part_kind='tool-return')], kind='request'), ModelResponse(parts=[ToolCallPart(tool_name='websearch_tool', args='{\"question\": \"types of RAG in retrieval augmented generation and Agentic RAG in production\"}', tool_call_id='call_ks5k', part_kind='tool-call')], model_name='llama-3.3-70b-versatile', timestamp=datetime.datetime(2025, 2, 22, 22, 27, 17, tzinfo=datetime.timezone.utc), kind='response'), ModelRequest(parts=[ToolReturnPart(tool_name='websearch_tool', content='Retrieval Augmented Generation (RAG) encompasses various techniques that enhance language models by integrating external knowledge sources. Traditional RAG models retrieve relevant information from a dataset and generate responses using a language model. Agentic RAG, however, includes a decision-making component that allows the system to identify additional sources, prioritize information, or initiate new queries based on user input. Types of RAG include Simple RAG, where the model retrieves information and generates responses, and Speculative RAG, which generates multiple responses and combines them for a unified view. Agentic RAG is particularly useful in production for personalized interactions and autonomous AI agents that can deliver context-aware responses.', tool_call_id='call_ks5k', timestamp=datetime.datetime(2025, 2, 22, 22, 27, 19, 592618, tzinfo=datetime.timezone.utc), part_kind='tool-return')], kind='request'), ModelResponse(parts=[TextPart(content='{\\n    \"Relevancy\": 0.8,\\n    \"Faithfulness\": 0.7,\\n    \"Context Quality\": 0.9,\\n    \"Needs Web Search\": true,\\n    \"Explanation\": \"The initial search using retriever_tool did not provide sufficient information, requiring a web search to gather more context on types of RAG and Agentic RAG in production.\",\\n    \"Answer\": \"There are several types of RAG, including Simple RAG and Speculative RAG. Agentic RAG can be used in production for personalized interactions and autonomous AI agents. It enhances the language model\\'s ability to identify additional sources, prioritize information, or initiate new queries based on user input, making it particularly useful for delivering context-aware responses.\"\\n}', part_kind='text')], model_name='llama-3.3-70b-versatile', timestamp=datetime.datetime(2025, 2, 22, 22, 27, 19, tzinfo=datetime.timezone.utc), kind='response')], _new_message_index=0, data='{\\n    \"Relevancy\": 0.8,\\n    \"Faithfulness\": 0.7,\\n    \"Context Quality\": 0.9,\\n    \"Needs Web Search\": true,\\n    \"Explanation\": \"The initial search using retriever_tool did not provide sufficient information, requiring a web search to gather more context on types of RAG and Agentic RAG in production.\",\\n    \"Answer\": \"There are several types of RAG, including Simple RAG and Speculative RAG. Agentic RAG can be used in production for personalized interactions and autonomous AI agents. It enhances the language model\\'s ability to identify additional sources, prioritize information, or initiate new queries based on user input, making it particularly useful for delivering context-aware responses.\"\\n}', _result_tool_name=None, _usage=Usage(requests=3, request_tokens=2251, response_tokens=216, total_tokens=2467, details=None))\n"
     ]
    }
   ],
   "source": [
    "query = \"how many types of RAG are available in retrieval augmented generation? how can we use Agentic RAG in production?\"\n",
    "try:\n",
    "    # The run_sync method expects a string prompt, not a Deps object\n",
    "    response = groq_agent.run_sync(\n",
    "        user_prompt=query,  # Pass the query string directly\n",
    "        deps=Deps(  # Pass Deps as a separate argument\n",
    "            question=query,\n",
    "            query=query,\n",
    "            context=\"\",\n",
    "            response=\"\",\n",
    "            content=query,\n",
    "            message=query,\n",
    "        ),\n",
    "    )\n",
    "    print(\"Response:\", response)\n",
    "except Exception as e:\n",
    "    print(f\"Error during agent execution: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "RunResult(_all_messages=[ModelRequest(parts=[SystemPromptPart(content='You are a Helpful Assistant proficient in providing concise, factual, and to-the-point answers for questions about RAG based on provided context.\\n\\nWhen answering questions:\\n1. First use the `retriever_tool` to get relevant context from the knowledge base\\n2. If the retrieved context is insufficient, use the `websearch_tool` to get additional information\\n3. Generate a comprehensive response based on all available context\\n\\nYour response should be in JSON format with the following structure:\\n{\\n    \"Relevancy\": <score 0-1>,\\n    \"Faithfulness\": <score 0-1>,\\n    \"Context Quality\": <score 0-1>,\\n    \"Needs Web Search\": True,  # Set to True when websearch_tool was used, False if not\\n    \"Explanation\": \"Explanation of the grading and search decision\",\\n    \"Answer\": \"Your detailed answer based on the available context\"\\n}\\n', dynamic_ref=None, part_kind='system-prompt'), UserPromptPart(content='how many types of RAG are available in retrieval augmented generation? how can we use Agentic RAG in production?', timestamp=datetime.datetime(2025, 2, 22, 22, 27, 15, 970896, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'), ModelResponse(parts=[ToolCallPart(tool_name='retriever_tool', args='{\"question\": \"types of RAG in retrieval augmented generation and Agentic RAG in production\"}', tool_call_id='call_vccx', part_kind='tool-call')], model_name='llama-3.3-70b-versatile', timestamp=datetime.datetime(2025, 2, 22, 22, 27, 16, tzinfo=datetime.timezone.utc), kind='response'), ModelRequest(parts=[ToolReturnPart(tool_name='retriever_tool', content='The design also incorporates the feedback from a diverse group of partici-\\npants during a workshop session, which focused on the practical aspects of imple-\\nmenting RAG systems. Their input highlighted the effectiveness of the systemâ€™s\\nreal-time retrieval capabilities, particularly in knowledge-intensive domains, and\\nunderscored the importance of refining the integration between retrieval and\\ngeneration to enhance the transparency and reliability of the systemâ€™s outputs.\\nThe design also incorporates the feedback from a diverse group of partici-\\npants during a workshop session, which focused on the practical aspects of imple-\\nmenting RAG systems. Their input highlighted the effectiveness of the systemâ€™s\\nreal-time retrieval capabilities, particularly in knowledge-intensive domains, and\\nunderscored the importance of refining the integration between retrieval and\\ngeneration to enhance the transparency and reliability of the systemâ€™s outputs.\\nThe design also incorporates the feedback from a diverse group of partici-\\npants during a workshop session, which focused on the practical aspects of imple-\\nmenting RAG systems. Their input highlighted the effectiveness of the systemâ€™s\\nreal-time retrieval capabilities, particularly in knowledge-intensive domains, and\\nunderscored the importance of refining the integration between retrieval and\\ngeneration to enhance the transparency and reliability of the systemâ€™s outputs.', tool_call_id='call_vccx', timestamp=datetime.datetime(2025, 2, 22, 22, 27, 16, 614098, tzinfo=datetime.timezone.utc), part_kind='tool-return')], kind='request'), ModelResponse(parts=[ToolCallPart(tool_name='websearch_tool', args='{\"question\": \"types of RAG in retrieval augmented generation and Agentic RAG in production\"}', tool_call_id='call_ks5k', part_kind='tool-call')], model_name='llama-3.3-70b-versatile', timestamp=datetime.datetime(2025, 2, 22, 22, 27, 17, tzinfo=datetime.timezone.utc), kind='response'), ModelRequest(parts=[ToolReturnPart(tool_name='websearch_tool', content='Retrieval Augmented Generation (RAG) encompasses various techniques that enhance language models by integrating external knowledge sources. Traditional RAG models retrieve relevant information from a dataset and generate responses using a language model. Agentic RAG, however, includes a decision-making component that allows the system to identify additional sources, prioritize information, or initiate new queries based on user input. Types of RAG include Simple RAG, where the model retrieves information and generates responses, and Speculative RAG, which generates multiple responses and combines them for a unified view. Agentic RAG is particularly useful in production for personalized interactions and autonomous AI agents that can deliver context-aware responses.', tool_call_id='call_ks5k', timestamp=datetime.datetime(2025, 2, 22, 22, 27, 19, 592618, tzinfo=datetime.timezone.utc), part_kind='tool-return')], kind='request'), ModelResponse(parts=[TextPart(content='{\\n    \"Relevancy\": 0.8,\\n    \"Faithfulness\": 0.7,\\n    \"Context Quality\": 0.9,\\n    \"Needs Web Search\": true,\\n    \"Explanation\": \"The initial search using retriever_tool did not provide sufficient information, requiring a web search to gather more context on types of RAG and Agentic RAG in production.\",\\n    \"Answer\": \"There are several types of RAG, including Simple RAG and Speculative RAG. Agentic RAG can be used in production for personalized interactions and autonomous AI agents. It enhances the language model\\'s ability to identify additional sources, prioritize information, or initiate new queries based on user input, making it particularly useful for delivering context-aware responses.\"\\n}', part_kind='text')], model_name='llama-3.3-70b-versatile', timestamp=datetime.datetime(2025, 2, 22, 22, 27, 19, tzinfo=datetime.timezone.utc), kind='response')], _new_message_index=0, data='{\\n    \"Relevancy\": 0.8,\\n    \"Faithfulness\": 0.7,\\n    \"Context Quality\": 0.9,\\n    \"Needs Web Search\": true,\\n    \"Explanation\": \"The initial search using retriever_tool did not provide sufficient information, requiring a web search to gather more context on types of RAG and Agentic RAG in production.\",\\n    \"Answer\": \"There are several types of RAG, including Simple RAG and Speculative RAG. Agentic RAG can be used in production for personalized interactions and autonomous AI agents. It enhances the language model\\'s ability to identify additional sources, prioritize information, or initiate new queries based on user input, making it particularly useful for delivering context-aware responses.\"\\n}', _result_tool_name=None, _usage=Usage(requests=3, request_tokens=2251, response_tokens=216, total_tokens=2467, details=None))"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. langchain JSON output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Relevancy\": 0.8,\n",
      "    \"Faithfulness\": 0.7,\n",
      "    \"Context Quality\": 0.9,\n",
      "    \"Needs Web Search\": true,\n",
      "    \"Explanation\": \"The initial search using retriever_tool did not provide sufficient information, requiring a web search to gather more context on types of RAG and Agentic RAG in production.\",\n",
      "    \"Answer\": \"There are several types of RAG, including Simple RAG and Speculative RAG. Agentic RAG can be used in production for personalized interactions and autonomous AI agents. It enhances the language model's ability to identify additional sources, prioritize information, or initiate new queries based on user input, making it particularly useful for delivering context-aware responses.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Relevancy': 0.8, 'Faithfulness': 0.7, 'Context Quality': 0.9, 'Needs Web Search': True, 'Explanation': 'The initial search using retriever_tool did not provide sufficient information, requiring a web search to gather more context on types of RAG and Agentic RAG in production.', 'Answer': \"There are several types of RAG, including Simple RAG and Speculative RAG. Agentic RAG can be used in production for personalized interactions and autonomous AI agents. It enhances the language model's ability to identify additional sources, prioritize information, or initiate new queries based on user input, making it particularly useful for delivering context-aware responses.\"}\n",
      "\n",
      "There are several types of RAG, including Simple RAG and Speculative RAG. Agentic RAG can be used in production for personalized interactions and autonomous AI agents. It enhances the language model's ability to identify additional sources, prioritize information, or initiate new queries based on user input, making it particularly useful for delivering context-aware responses.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "print(parser.parse(response.data), end=\"\\n\\n\")\n",
    "print(parser.parse(response.data)[\"Answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved context: mentioned. This contribution is practical, as it helps practitioners implement\n",
      "RAG models to address real world challenges with dynamic data and improved\n",
      "accuracy. The guide provides users clear, acti...\n",
      "{\"Relevancy\": 0.8, \"Faithfulness\": 0.7, \"Context Quality\": 0.6, \"Needs Web Search\": false, \"Explanation\": \"The retrieved context mentions RAG models and their application in real-world challenges, but does not provide a clear definition or explanation of RAG in AI/ML. The context is somewhat relevant but lacks clarity and detail.\", \"Answer\": \"RAG in AI/ML refers to Retrieval-Augmented Generation models, which are a type of artificial intelligence model that combines retrieval and generation capabilities to improve the accuracy and effectiveness of natural language processing tasks. These models are designed to retrieve relevant information from a knowledge base or database and use that information to generate more accurate and informative responses to user queries. RAG models have the potential to improve the performance of a wide range of NLP tasks, including question answering, text summarization, and conversational dialogue systems.\"}\n",
      "{'Relevancy': 0.8, 'Faithfulness': 0.7, 'Context Quality': 0.6, 'Needs Web Search': False, 'Explanation': 'The retrieved context mentions RAG models and their application in real-world challenges, but does not provide a clear definition or explanation of RAG in AI/ML. The context is somewhat relevant but lacks clarity and detail.', 'Answer': 'RAG in AI/ML refers to Retrieval-Augmented Generation models, which are a type of artificial intelligence model that combines retrieval and generation capabilities to improve the accuracy and effectiveness of natural language processing tasks. These models are designed to retrieve relevant information from a knowledge base or database and use that information to generate more accurate and informative responses to user queries. RAG models have the potential to improve the performance of a wide range of NLP tasks, including question answering, text summarization, and conversational dialogue systems.'}\n",
      "RAG in AI/ML refers to Retrieval-Augmented Generation models, which are a type of artificial intelligence model that combines retrieval and generation capabilities to improve the accuracy and effectiveness of natural language processing tasks. These models are designed to retrieve relevant information from a knowledge base or database and use that information to generate more accurate and informative responses to user queries. RAG models have the potential to improve the performance of a wide range of NLP tasks, including question answering, text summarization, and conversational dialogue systems.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is RAG in AI/ML?\"\n",
    "response = groq_agent.run_sync(query)\n",
    "print(response.data)\n",
    "print(parser.parse(response.data))\n",
    "print(parser.parse(response.data)[\"Answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved context: You are an expert assistant with access to the\n",
      "following context extracted from documents . Your\n",
      "job is to answer the user â€™s question as accurately\n",
      "as possible , using the context below .\n",
      "Context :\n",
      "{...\n",
      "Retrieved context: You are an expert assistant with access to the\n",
      "following context extracted from documents . Your\n",
      "job is to answer the user â€™s question as accurately\n",
      "as possible , using the context below .\n",
      "Context :\n",
      "{...\n",
      "Web search result: Louisville is a city in Jefferson County, Kentucky, situated on the banks of the Ohio River. It is located at the Falls of the Ohio River and serves as the largest city in Kentucky and the seat of Jef...\n",
      "{\n",
      "    \"Relevancy\": 0.8,\n",
      "    \"Faithfulness\": 0.9,\n",
      "    \"Context Quality\": 0.7,\n",
      "    \"Needs Web Search\": true,\n",
      "    \"Explanation\": \"The information about Louisville city location was not available in the provided context, so a web search was performed to find the answer.\",\n",
      "    \"Answer\": \"Louisville city is located in Jefferson County, Kentucky, on the banks of the Ohio River.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "query = \"Where is Louisville city Located?\"\n",
    "response = groq_agent.run_sync(query)\n",
    "print(response.data)\n",
    "# print(parser.parse(response.data))\n",
    "# print(parser.parse(response.data)[\"Answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
