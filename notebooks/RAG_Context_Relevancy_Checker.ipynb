{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG_Context_Relevancy_Checker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [RAG_Context_Relevancy_Checker](https://medium.com/the-ai-forum/rag-context-relevancy-checker-agent-using-deepseek-r1-70b-on-groq-modernbert-langchain-58edb6b0f29c)\n",
    "- [Paper: Developing Retrieval Augmented Generation (RAG) based LLM Systems from PDFs: An Experience Report](https://arxiv.org/pdf/2410.15944v1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"HF_TOKEN\"] = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain, LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_judge = ChatGroq(model=\"deepseek-r1-distill-llama-70b\")\n",
    "llm_rag = ChatGroq(model=\"mixtral-8x7b-32768\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_judge.verbose = True\n",
    "llm_rag.verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-15 16:32:15--  https://arxiv.org/pdf/2410.15944v1\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.3.42, 151.101.131.42, 151.101.195.42, ...\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.3.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 749669 (732K) [application/pdf]\n",
      "Saving to: â€˜../data/RAG.pdfâ€™\n",
      "\n",
      "../data/RAG.pdf     100%[===================>] 732.10K  1.09MB/s    in 0.7s    \n",
      "\n",
      "2025-02-15 16:32:16 (1.09 MB/s) - â€˜../data/RAG.pdfâ€™ saved [749669/749669]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://arxiv.org/pdf/2410.15944v1\" -O ../data/RAG.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "{'source': '../data/RAG.pdf', 'file_path': '../data/RAG.pdf', 'page': 0, 'total_pages': 36, 'Author': '', 'CreationDate': 'D:20241022015619Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20241022015619Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PDFPlumberLoader\n",
    "\n",
    "loader = PDFPlumberLoader(\"../data/RAG.pdf\")\n",
    "\n",
    "docs = loader.load()\n",
    "print(len(docs))\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n",
      "Developing Retrieval Augmented Generation\n",
      "(RAG) based LLM Systems from PDFs: An\n",
      "Experience Report\n",
      "Ayman Asad Khan Md Toufique Hasan\n",
      "Tampere University Tampere University\n",
      "ayman.khan@tuni.fi mdtoufique.hasan@tuni.fi\n",
      "Kai Kristian Kemell Jussi Rasku Pekka Abrahamsson\n",
      "Tampere University Tampere University Tampere University\n",
      "kai-kristian.kemell@tuni.fi jussi.rasku@tuni.fi pekka.abrahamsson@tuni.fi\n",
      "Abstract. This paper presents an experience report on the develop-\n",
      "ment of Retrieval Augmented Generation (RAG) systems using PDF\n",
      "documentsastheprimarydatasource.TheRAGarchitecturecombines\n",
      "generativecapabilitiesofLargeLanguageModels(LLMs)withthepreci-\n",
      "sionofinformationretrieval.Thisapproachhasthepotentialtoredefine\n",
      "how we interact with and augment both structured and unstructured\n",
      "knowledge in generative models to enhance transparency, accuracy and\n",
      "contextuality of responses. The paper details the end-to-end pipeline,\n",
      "from data collection, preprocessing, to retrieval indexing and response\n",
      "generation,highlightingtechnicalchallengesandpracticalsolutions.We\n",
      "aim to offer insights to researchers and practitioners developing similar\n",
      "systems using two distinct approaches: OpenAIâ€™s Assistant API with\n",
      "GPTSeries andLlamaâ€™sopen-sourcemodels.Thepracticalimplications\n",
      "of this research lie in enhancing the reliability of generative AI systems\n",
      "in various sectors where domain specific knowledge and real time infor-\n",
      "mationretrievalisimportant.ThePythoncodeusedinthisworkisalso\n",
      "available at: GitHub. Keywords: Retrieval Augmented Generation (RAG), Large Language Models\n",
      "(LLMs), Generative AI in Software Development, Transparent AI. 1 Introduction\n",
      "Large language models (LLMs) excel at generating human like responses, but\n",
      "base AI models canâ€™t keep up with the constantly evolving information within\n",
      "dynamicsectors.Theyrelyonstatictrainingdata,leadingtooutdatedorincom-\n",
      "plete answers.\n"
     ]
    }
   ],
   "source": [
    "# chunk documents\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "# set the embeddings model\n",
    "model_name = \"nomic-ai/modernbert-embed-base\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": False}\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "text_splitter = SemanticChunker(embedding_model)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "print(len(documents))\n",
    "print(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VECTOR STORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "    collection_name=\"deepseek_collection\",\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=\"../data/chroma_langchain_db\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add embeddings to vector store\n",
    "vector_store.add_documents(documents=documents)\n",
    "len(vector_store.get()[\"documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the retriever\n",
    "retriver = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='be70672d-0891-4a7b-a555-d8b6c8211703', metadata={'Author': '', 'CreationDate': 'D:20241022015619Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20241022015619Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'file_path': '../data/RAG.pdf', 'page': 31, 'source': '../data/RAG.pdf', 'total_pages': 36}, page_content='Fig.6: Most Valuable Aspects of the Workshop. implementation of RAG systems.'),\n",
       " Document(id='04d47e53-02c4-439d-8ba6-fa89d18bffd6', metadata={'Author': '', 'CreationDate': 'D:20241022015619Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20241022015619Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'file_path': '../data/RAG.pdf', 'page': 2, 'source': '../data/RAG.pdf', 'total_pages': 36}, page_content='enhancing the modelâ€™s ability to respond. Fig.1: Architecture of Retrieval Augmented Generation(RAG) system. 2.'),\n",
       " Document(id='ddb30388-e750-4d7b-a18a-d8f32b530e46', metadata={'Author': '', 'CreationDate': 'D:20241022015619Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20241022015619Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'file_path': '../data/RAG.pdf', 'page': 9, 'source': '../data/RAG.pdf', 'total_pages': 36}, page_content='The design also incorporates the feedback from a diverse group of partici-\\npantsduringaworkshopsession,whichfocusedonthepracticalaspectsofimple-\\nmenting RAG systems. Their input highlighted the effectiveness of the systemâ€™s\\nreal-timeretrievalcapabilities,particularlyinknowledge-intensivedomains,and\\nunderscored the importance of refining the integration between retrieval and\\ngeneration to enhance the transparency and reliability of the systemâ€™s outputs. This design sets the foundation for a RAG system capable of addressing the\\nneeds of domains requiring precise, up-to-date information. 4 Results: Step-by-Step Guide to RAG\\n4.1 Setting Up the Environment\\nThis section walks you through the steps required to set up a development\\nenvironmentforRetrievalAugmentedGeneration(RAG)onyourlocalmachine. We will cover the installation of Python, setting up a virtual environment and\\nconfiguring an IDE (VSCode).'),\n",
       " Document(id='89029a86-3f8f-4fb2-885f-fffc518efcb0', metadata={'Author': '', 'CreationDate': 'D:20241022015619Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20241022015619Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'file_path': '../data/RAG.pdf', 'page': 30, 'source': '../data/RAG.pdf', 'total_pages': 36}, page_content='Fig.4: Participantsâ€™ Familiarity with RAG Systems. Priortoattendingtheworkshop,themajorityofparticipantsreportedarea-\\nsonable level of familiarity with RAG systems.This indicated that the audience\\nhadafoundationalunderstandingoftheconceptspresented,allowingformorein\\ndepthdiscussionsduringtheworkshop.Aftertheworkshop,therewasanotable\\nimprovement in participantsâ€™ understanding of RAG systems. Fig.5: Participantsâ€™ Improvement in Understanding RAG Systems. Themajorityofparticipantshighlightedthepracticalcodingexercisesasthe\\nmostvaluableaspectoftheworkshop,whichhelpedthembetterunderstandthe\\n31\\n'),\n",
       " Document(id='0431ec74-e00c-4a01-ad63-9262768eea5d', metadata={'Author': '', 'CreationDate': 'D:20241022015619Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20241022015619Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'Producer': 'pdfTeX-1.40.25', 'Subject': '', 'Title': '', 'Trapped': 'False', 'file_path': '../data/RAG.pdf', 'page': 32, 'source': '../data/RAG.pdf', 'total_pages': 36}, page_content='RAG\\nmodels provide practical solutions with pulling in real time data from provided\\nsources. The ability to explain and trace how RAG models reach their answers\\nalsobuildstrustwhereaccountabilityanddecisionmakingbasedonrealevidence\\nis important. Inthispaper,wedevelopedaRAGguidethatwetestedinaworkshopsetting,\\nwhere participants set up and deployed RAG systems following the approaches\\nmentioned. This contribution is practical, as it helps practitioners implement\\nRAG models to address real world challenges with dynamic data and improved\\naccuracy.Theguideprovidesusersclear,actionablestepstointegrateRAGinto\\ntheir workflows, contributing to the growing toolkit of AI driven solutions. With that, RAG also opens new research avenues that can shape the future\\nofAIandNLPtechnologies.Asthesemodelsandtoolsimprove,therearemany\\npotentialareasforgrowth,suchasfindingbetterwaystosearchforinformation,\\nadapting to new data automatically, and handling more than just text (like\\nimages or audio). Recent advancements in tools and technologies have further\\naccelerated the development and deployment of RAG models. As RAG models\\ncontinue to evolve, several emerging trends are shaping the future of this field. 1. Haystack: An open-source framework that integrates dense and sparse re-\\ntrieval methods with large-scale language models. Haystack supports real-\\ntime search applications and can be used to develop RAG models that per-\\nform tasks such as document retrieval, question answering, and summariza-\\ntion [4]. 2. Elasticsearch with Vector Search: Enhanced support for dense vector\\nsearch capabilities, allowing RAG models to perform more sophisticated re-\\ntrieval tasks. Elasticsearchâ€™s integration with frameworks like Faiss enables\\n33\\n')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriver.invoke(\"what is RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROMPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevancy_prompt = \"\"\"You are an expert judge tasked with evaluating whather the  EACH OF THE CONTEXT provided in the CONTEXT LIST is self sufficient to answer the QUERY asked.\n",
    "Analyze the provided QUERY AND  CONTEXT to determine if each Ccontent in the CONTEXT LIST contains Relevant information to answer the QUERY.\n",
    "\n",
    "Guidelines:\n",
    "1. The content must not introduce new information beyond what's provided in the QUERY.\n",
    "2. Pay close attention to the subject of statements. Ensure that attributes, actions, or dates are correctly associated with the right entities (e.g., a person vs. a TV show they star in).\n",
    "6. Be vigilant for subtle misattributions or conflations of information, even if the date or other details are correct.\n",
    "7. Check that the content in the CONTEXT LIST doesn't oversimplify or generalize information in a way that changes the meaning of the QUERY.\n",
    "\n",
    "Analyze the text thoroughly and assign a relevancy score 0 or 1 where:\n",
    "- 0: The content has all the necessary information to answer the QUERY\n",
    "- 1: The content does not has the necessary information to answer the QUERY\n",
    "\n",
    "```\n",
    "EXAMPLE:\n",
    "\n",
    "INPUT (for context only, not to be used for faithfulness evaluation):\n",
    "What is the capital of France?\n",
    "\n",
    "CONTEXT:\n",
    "['France is a country in Western Europe. Its capital is Paris, which is known for landmarks like the Eiffel Tower.',\n",
    "'Mr. Naveen patnaik has been the chief minister of Odisha for consequetive 5 terms']\n",
    "\n",
    "OUTPUT:\n",
    "The Context has sufficient information to answer the query.\n",
    "\n",
    "RESPONSE:\n",
    "{{\"score\":0}}\n",
    "```\n",
    "\n",
    "CONTENT LIST:\n",
    "{context}\n",
    "\n",
    "QUERY:\n",
    "{retriever_query}\n",
    "Provide your verdict in JSON format  with a single key 'score' and no preamble or explanation:\n",
    "[{{\"content:1,\"score\": <your score either 0 or 1>,\"Reasoning\":<why you have chose the score as 0 or 1>}},\n",
    "{{\"content:2,\"score\": <your score either 0 or 1>,\"Reasoning\":<why you have chose the score as 0 or 1>}},\n",
    "...]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "context_relevancy_checker_prompt = PromptTemplate(\n",
    "    input_variables=[\"retriever_query\", \"context\"], template=relevancy_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant Context Picker Agent\n",
    "relevant_prompt = PromptTemplate(\n",
    "    input_variables=[\"relevancy_response\"],\n",
    "    template=\"\"\"\n",
    "    You main task is to analyze the json structure as a part of the Relevancy Response.\n",
    "    Review the Relevancy Response and do the following:-\n",
    "    (1) Look at the Json Structure content\n",
    "    (2) Analyze the 'score' key in the Json Structure content.\n",
    "    (3) pick the value of 'content' key against those 'score' key value which has 0.\n",
    "    .\n",
    "\n",
    "    Relevancy Response:\n",
    "    {relevancy_response}\n",
    "\n",
    "    Provide your verdict in JSON format  with a single key 'content number' and no preamble or explanation:\n",
    "    [{{\"content\":<content number>}}]\n",
    "\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MeaningFul Context for Response synthesis Agent\n",
    "context_prompt = PromptTemplate(\n",
    "    input_variables=[\"context_number\"],\n",
    "    template=\"\"\"\n",
    "    Your main task is to analyze the json structure as a part of the Context Number Response and the list of Contexts provided in the 'Content List' and perform the following steps:-\n",
    "    (1) Look at the output from the Relevant Context Picker Agent.\n",
    "    (2) Analyze the 'content' key in the Json Structure format({{\"content\":<<content_number>>}}).\n",
    "    (3) Retrieve the value of 'content' key and pick up the context corresponding to that element from the Content List provided.\n",
    "    (4) Pass the retrieved context for each corresponing element number referred in the 'Context Number Response'\n",
    "\n",
    "    Context Number Response:\n",
    "    {context_number}\n",
    "\n",
    "    Content List:\n",
    "    {context}\n",
    "\n",
    "    Provide your verdict in JSON format  with a two key 'relevant_content' and 'context_number' no preamble or explanation:\n",
    "    [{{\"context_number\":<content1>,\"relevant_content\":<content corresponing to that element 1 in the Content List>}},\n",
    "    {{\"context_number\":<content4>,\"relevant_content\":<content corresponing to that element 4 in the Content List>}},\n",
    "    ...\n",
    "    ]\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated Final Response Chain with RAG-specific prompt\n",
    "final_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"relevant_contexts\"],\n",
    "    template=\"\"\"You are a helpful assistant very proficient in formulating clear and meaningful answers from the context provided.\n",
    "\n",
    "SYSTEM INSTRUCTIONS:\n",
    "1. Use ONLY the information from the provided relevant contexts to formulate your response\n",
    "2. Provide clear, concise, and meaningful answers\n",
    "3. If the contexts don't contain sufficient information, respond with 'I do not know'\n",
    "4. Maintain accuracy while synthesizing information\n",
    "5. Use natural, coherent language\n",
    "6. Avoid making up information not present in the contexts\n",
    "\n",
    "QUERY:\n",
    "{query}\n",
    "\n",
    "RELEVANT CONTEXTS:\n",
    "{relevant_contexts}\n",
    "\n",
    "Based on the above information, provide a clear and concise response. Remember:\n",
    "- Only use information from the provided contexts\n",
    "- Keep the response focused and meaningful\n",
    "- Say 'I do not know' if contexts are insufficient\n",
    "- Do not make up or infer information not present in the contexts\n",
    "\n",
    "ANSWER:\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain, LLMChain\n",
    "\n",
    "#\n",
    "context_relevancy_evaluation_chain = LLMChain(\n",
    "    llm=llm_judge,\n",
    "    prompt=context_relevancy_checker_prompt,\n",
    "    output_key=\"relevancy_response\",\n",
    ")\n",
    "#\n",
    "pick_relevant_context_chain = LLMChain(\n",
    "    llm=llm_judge, prompt=relevant_prompt, output_key=\"context_number\"\n",
    ")\n",
    "#\n",
    "relevant_contexts_chain = LLMChain(\n",
    "    llm=llm_judge, prompt=context_prompt, output_key=\"relevant_contexts\"\n",
    ")\n",
    "\n",
    "response_chain = LLMChain(llm=llm_rag, prompt=final_prompt, output_key=\"final_response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated Sequential Chain with all outputs\n",
    "context_management_chain = SequentialChain(\n",
    "    chains=[\n",
    "        context_relevancy_evaluation_chain,\n",
    "        pick_relevant_context_chain,\n",
    "        relevant_contexts_chain,\n",
    "        response_chain,\n",
    "    ],\n",
    "    input_variables=[\"context\", \"retriever_query\", \"query\"],\n",
    "    output_variables=[\n",
    "        \"relevancy_response\",\n",
    "        \"context_number\",\n",
    "        \"relevant_contexts\",\n",
    "        \"final_response\",\n",
    "    ],\n",
    "    verbose=True,  # Set to False in production\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with retriever\n",
    "def get_rag_response(query, retriever):\n",
    "    # Get contexts from retriever\n",
    "    contexts = retriever.invoke(query)\n",
    "    context = [d.page_content for d in contexts]\n",
    "\n",
    "    # Run the chain\n",
    "    final_output = context_management_chain(\n",
    "        {\"context\": context, \"retriever_query\": query, \"query\": query}\n",
    "    )\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/43/572h075x24q9rq1slmdfw9yw0000gn/T/ipykernel_48534/3225485183.py:8: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  final_output = context_management_chain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"What is RAG?\"\n",
    "final_output = get_rag_response(query, retriver)\n",
    "\n",
    "# Access individual outputs\n",
    "relevancy_response = final_output[\"relevancy_response\"]\n",
    "context_number = final_output[\"context_number\"]\n",
    "relevant_contexts = final_output[\"relevant_contexts\"]\n",
    "final_response = final_output[\"final_response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------- ðŸŸ¥ context_relevancy_evaluation_chain Statement ðŸŸ¥ --------\n",
      "\n",
      "<think>\n",
      "Alright, I need to determine if each content in the provided context list sufficiently answers the query \"What is RAG?\" I'll evaluate each content one by one based on the guidelines given.\n",
      "\n",
      "First, the query is asking for a definition or explanation of RAG. So, I'm looking for content that clearly defines RAG, perhaps by expanding the acronym and explaining its components.\n",
      "\n",
      "Looking at content 1: It mentions \"implementation of RAG systems\" and \"Retrieval Augmented Generation (RAG) system.\" It includes Figure 1 about the architecture, which might visually explain RAG, but without the figure, the text only refers to RAG without defining it. So, this might not be sufficient on its own.\n",
      "\n",
      "Content 2: It talks about enhancing the modelâ€™s ability to respond and refers to Figure 1 again. It doesn't provide a definition of RAG, just mentions it in the context of architecture. So, it's similar to content 1 and probably insufficient.\n",
      "\n",
      "Content 3: Discusses participants' familiarity with RAG systems and improvement after a workshop. It doesn't define RAG but mentions it in the context of understanding. Still, no explicit definition here.\n",
      "\n",
      "Content 4: This content is more detailed. It mentions the design incorporating feedback about RAG systems, focusing on real-time retrieval capabilities and integration between retrieval and generation. It also sets the foundation for RAG systems in knowledge-intensive domains. While it explains aspects of RAG, it doesn't explicitly define it for someone who doesn't know what RAG is.\n",
      "\n",
      "Content 5: This section is about setting up a development environment for RAG. It explains the process but doesn't define what RAG is. It assumes prior knowledge.\n",
      "\n",
      "Content 6: This is the most promising. It starts by saying RAG models provide practical solutions by pulling real-time data. It explains that RAG stands for Retrieval Augmented Generation and mentions that they developed a RAG guide tested in a workshop. This content directly explains what RAG is and its purpose, making it sufficient to answer the query.\n",
      "\n",
      "So, evaluating each content:\n",
      "\n",
      "1. Doesn't define RAG, just refers to it.\n",
      "2. Same as 1, focuses on architecture without defining.\n",
      "3. Talks about understanding RAG but doesn't define it.\n",
      "4. Explains aspects but no definition.\n",
      "5. Setup process, no definition.\n",
      "6. Provides a clear definition and explanation.\n",
      "\n",
      "Therefore, only content 6 has the necessary information to answer the query. The others either refer to RAG without defining it or discuss related aspects without explaining what RAG is.\n",
      "</think>\n",
      "\n",
      "{\"content\":1,\"score\":1,\"Reasoning\":\"The content refers to RAG systems and their implementation but does not provide a definition or explanation of what RAG is.\"}\n",
      "\n",
      "{\"content\":2,\"score\":1,\"Reasoning\":\"The content mentions enhancing the model's ability to respond and refers to the architecture of a RAG system but does not define what RAG is.\"}\n",
      "\n",
      "{\"content\":3,\"score\":1,\"Reasoning\":\"The content discusses participants' familiarity with RAG systems and their improvement in understanding but does not define what RAG is.\"}\n",
      "\n",
      "{\"content\":4,\"score\":1,\"Reasoning\":\"The content talks about the design of RAG systems and their applications but does not explicitly define what RAG is.\"}\n",
      "\n",
      "{\"content\":5,\"score\":1,\"Reasoning\":\"The content provides a step-by-step guide to setting up a RAG system but does not define what RAG is.\"}\n",
      "\n",
      "{\"content\":6,\"score\":0,\"Reasoning\":\"The content explicitly defines RAG as Retrieval Augmented Generation and explains its purpose and functionality, making it sufficient to answer the query.\"}\n",
      "\n",
      "-------- ðŸŸ¦ pick_relevant_context_chain Statement ðŸŸ¦  --------\n",
      "\n",
      "<think>\n",
      "Alright, so I need to figure out which content number is the correct one to provide as the answer. From the previous analysis, it seems that only content 6 actually defines what RAG is. The other contents mention RAG in different contexts but don't provide a clear definition. Since the task is to pick the content where the 'score' is 0, and in the JSON output, content 6 has a score of 0, that must be the one I need to select.\n",
      "\n",
      "So, the correct content number is 6.\n",
      "</think>\n",
      "\n",
      "{\"content\":6}\n",
      "\n",
      "-------- ðŸŸ¥ relevant_contexts_chain Statement ðŸŸ¥ --------\n",
      "\n",
      "<think>\n",
      "Alright, I'm trying to figure out how to solve this problem. Let me start by understanding what the user is asking for. They provided a JSON structure called \"Context Number Response\" and a list of contexts in the \"Content List.\" My task is to analyze the JSON, extract the 'content' key values, and retrieve the corresponding contexts from the Content List.\n",
      "\n",
      "First, I look at the Context Number Response. It's a JSON array where each object has a 'score' and a 'content' key. My job is to pick the 'content' values where the 'score' is 0. In this case, the only content number with a score of 0 is 6.\n",
      "\n",
      "Next, I need to find the context corresponding to content number 6 in the Content List. The Content List is a list where each element's index plus one corresponds to the content number. So, content 6 would be the 5th element in the list (since indexing starts at 0).\n",
      "\n",
      "Looking at the Content List, the 6th content (index 5) is about RAG models providing practical solutions by pulling real-time data, explaining how they reach their answers, and building trust. It also mentions a workshop where participants set up and deployed RAG systems, contributing to AI-driven solutions.\n",
      "\n",
      "So, I need to create a JSON array where each object has 'context_number' and 'relevant_content'. Since only content 6 is relevant here, the array will have one object with context_number 6 and the corresponding content from the list.\n",
      "\n",
      "I should make sure that the JSON is correctly formatted, with proper commas and brackets. No additional explanations or text should be included, just the JSON output.\n",
      "\n",
      "Putting it all together, the final JSON should look like this:\n",
      "</think>\n",
      "\n",
      "{\"context_number\":6,\"relevant_content\":\"RAG models provide practical solutions with pulling in real time data from provided sources. The ability to explain and trace how RAG models reach their answers also builds trust where accountability and decisionmaking based on real evidence is important. In this paper, we developed a RAG guide that we tested in a workshop setting, where participants set up and deployed RAG systems following the approaches mentioned. This contribution is practical, as it helps practitioners implement RAG models to address real world challenges with dynamic data and improved accuracy. The guide provides users clear, actionable steps to integrate RAG into their workflows, contributing to the growing toolkit of AI driven solutions. With that, RAG also opens new research avenues that can shape the future of AI and NLP technologies. As these models and tools improve, there are many potential areas for growth, such as finding better ways to search for information, adapting to new data automatically, and handling more than just text (like images or audio). Recent advancements in tools and technologies have further accelerated the development and deployment of RAG models. As RAG models continue to evolve, several emerging trends are shaping the future of this field. 1. Haystack: An open-source framework that integrates dense and sparse retrieval methods with large-scale language models. Haystack supports real-time search applications and can be used to develop RAG models that perform tasks such as document retrieval, question answering, and summarization [4]. 2. Elasticsearch with Vector Search: Enhanced support for dense vector search capabilities, allowing RAG models to perform more sophisticated retrieval tasks. Elasticsearchâ€™s integration with frameworks like Faiss enables\"}\n",
      "\n",
      "-------- ðŸŸ¥ Rag Response Statement ðŸŸ¥ --------\n",
      "\n",
      "RAG stands for Retrieval-Augmented Generation. It is a model that provides practical solutions by pulling real-time data from provided sources, explaining how it reaches its answers, and building trust where accountability and decision-making based on real evidence is important. RAG models can be used in various applications such as document retrieval, question answering, and summarization. Recently, there have been advancements in tools and technologies that have accelerated the development and deployment of RAG models.\n"
     ]
    }
   ],
   "source": [
    "# Print first response (RAG Relevan Context Picker Agent)\n",
    "print(\"\\n-------- ðŸŸ¥ context_relevancy_evaluation_chain Statement ðŸŸ¥ --------\\n\")\n",
    "print(final_output[\"relevancy_response\"])\n",
    "\n",
    "# Print final legally refined response\n",
    "print(\"\\n-------- ðŸŸ¦ pick_relevant_context_chain Statement ðŸŸ¦  --------\\n\")\n",
    "print(final_output[\"context_number\"])\n",
    "\n",
    "print(\"\\n-------- ðŸŸ¥ relevant_contexts_chain Statement ðŸŸ¥ --------\\n\")\n",
    "print(final_output[\"relevant_contexts\"])\n",
    "\n",
    "print(\"\\n-------- ðŸŸ¥ Rag Response Statement ðŸŸ¥ --------\\n\")\n",
    "print(final_output[\"final_response\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
